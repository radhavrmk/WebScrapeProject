summary(df)
describe(df)
hist(df$Body.Temp)
hist(df$Heart.Rate)
plot(df$Body.Temp, df$Heart.Rate)
corr(df$Body.Temp, df$Heart.Rate)
corr(df[,-2])
cor(df$Body.Temp, df$Heart.Rate)
cor(df[,-2])
df %>% ggplot(aes(x=Gender, y=Body.Temp, fill=Gender)) +
geom_boxplot()
df %>% ggplot(aes(x=Gender, y=Heart.Rate, fill=Gender)) +
geom_boxplot()
summary_df = df %>% group_by(Gender) %>%
summarise(avg_temp = mean(Body.Temp), avg_HR = mean(Heart.Rate))
summary_df
df %>% ggplot(aes(x=Body.Temp, y =Heart.Rate, color=Gender)) +
geom_point() +
geom_point(aes(x=summary_df[[2]][1], y=summary_df[[3]][1]), size=4, color="red") +
geom_point(aes(x=summary_df[[2]][2], y=summary_df[[3]][2]), size=4, color="darkgreen")
df %>% ggplot(aes(y=Body.Temp)) +
geom_boxplot(fill="grey") +
geom_abline(slope = 0, intercept = 98.6, color = "red")
df %>% ggplot(aes(y=Body.Temp, x = Gender)) +
geom_boxplot(fill="grey") +
geom_abline(slope = 0, intercept = 98.6, color = "red")
t.test(temp$Body.Temp, mu = 98.6)
t.test(df$Body.Temp)$conf.int
t.test(df$Body.Temp, mu = 98.6)
t.test(df$Body.Temp)$conf.int
calcConfInterval =  function(data, alpha){
mu = mean(data)
sigma = sd(data)/sqrt(length(data))
left =  mu - (qnorm(1-(alpha/2)) * sigma)
right =  mu + (qnorm(1-(alpha/2)) * sigma)
return(c(left,right))
}
calcConfInterval(df$Body.Temp,0.05)
male_temp = df$Body.Temp[df$Gender=="Male"]
female_temp = df$Body.Temp[df$Gender=="Female"]
t.test(male_temp)
t.test(female_temp)
t.test(male_temp, female_temp)
t.test(male_temp, conf.level = 0.9)
t.test(df$Body.Temp, mu = 98.6)
t.test(male_temp, conf.level = 0.9)
shiny::runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('SemanticExample_bus')
runApp('SemanticExample2')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
shiny::runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
shiny::runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
library(RColorBrewer)
n <- 60
qual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]
col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))
pie(rep(1,n), col=sample(col_vector, n))
library(randomcoloR)
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
a = c(2,3,4,5,6)
count(a)
length(a)
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
x = matrix(range(9),nrow = 3)
x
a = range(9)
a
a = range(1:9)
a
a = c(1:10)
a
a = c(1:10,2)
a
x = matrix(1:9,nrow = 3)
x
x[1,]
x[1:2,]
x[1:2,3]
shiny::runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
runApp('~/Documents/Radha/OneDrive - Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/RetailTrendsShiney/RetailSales')
installed.packages("VIM")
install.packages("VIM")
install.packages("VIM")
help(sleep) #Inspecting the mammal sleep dataset.
sleep
summary(sleep) #Summary information for the sleep dataset.
summary(sleep) #Summary information for the sleep dataset.
sleep
summary(sleep) #Summary information for the sleep dataset.
sapply(sleep, sd) #Standard deviations for the sleep dataset; any issues?
VIM::aggr(sleep) #A graphical interpretation of the missing values and their
library(mice) #Load the multivariate imputation by chained equations library.
install.packages("mice")
dim(sleep)
sum(is.na(sleep))
?md.pattern
??md.pattern
library(mice) #Load the multivariate imputation by chained equations library.
?md.pattern
mice::md.pattern(sleep) #Can also view this information from a data perspective.
library(mice) #Load the multivariate imputation by chained equations library.
library(mice) #Load the multivariate imputation by chained equations library.
dim(sleep)
sum(is.na(sleep))
?md.pattern
mice::md.pattern(sleep) #Can also view this information from a data perspective.
###############################
#####Mean Value Imputation#####
###############################
#Creating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
mean(missing.data$x2, na.rm = TRUE) #Mean of x2 prior to imputation.
sd(missing.data$x2, na.rm = TRUE) #Standard deviation of x2 prior to imputation.
mice::md.pattern(missing.data)
mean(missing.data$x2, na.rm = TRUE) #Mean of x2 prior to imputation.
sd(missing.data$x2, na.rm = TRUE) #Standard deviation of x2 prior to imputation.
cor(missing.data, use = "complete.obs") #Correlation prior to imputation.
#Mean value imputation method 1.
missing.data$x2[is.na(missing.data$x2)] = mean(missing.data$x2, na.rm=TRUE)
missing.data
#Mean value imputation method 2.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
missing.data = transform(missing.data, x2 = ifelse(is.na(x2),
mean(x2, na.rm=TRUE),
x2))
missing.data
#Mean value imputation method 3.
library(caret)
install.packages("caret")
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = caret::preProcess(missing.data, method = "medianImpute")
missing.data = predict(pre, missing.data)
missing.data
### Why Caret?
## 1. Maintain the structure of train - predict as other machine learning procedure.
##    This is particularly important when impute for future observation
## 2. Can be collected with other preprocesses, as below:
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = preProcess(missing.data, method = c("scale", "medianImpute"))
missing.data = predict(pre, missing.data)
missing.data
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = preProcess(missing.data, method = c("center","scale", "medianImpute"))
missing.data = predict(pre, missing.data)
#Mean value imputation method 3.
library(caret)
### Why Caret?
## 1. Maintain the structure of train - predict as other machine learning procedure.
##    This is particularly important when impute for future observation
## 2. Can be collected with other preprocesses, as below:
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = preProcess(missing.data, method = c("scale", "medianImpute"))
missing.data = predict(pre, missing.data)
missing.data
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = preProcess(missing.data, method = c("center","scale", "medianImpute"))
missing.data = predict(pre, missing.data)
missing.data
## manual scale
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
scaled = mapply(FUN = '/',missing.data,sapply(missing.data, function(x) {sd(x,na.rm=T)}))
scaled
install.packages("Hmisc")
#Mean value imputation method 4.
library(Hmisc) #Load the Harrell miscellaneous library.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
imputed.x2 = impute(missing.data$x2, mean) #Specifically calling the x2 variable.
imputed.x2
summary(imputed.x2) #Summary information for the imputed variable.
is.imputed(imputed.x2) #Boolean vector indicating imputed values.
imputed.x2
summary(imputed.x2) #Summary information for the imputed variable.
library(dplyr)
glimpse(imputed.x2)
describe(imputed.x2)
type(imputed.x2)
typeof(imputed.x2)
class(imputed.x2)
class(imputed)
glimpse(imputed)
imputed.x2
summary(imputed.x2) #Summary information for the imputed variable.
is.imputed(imputed.x2) #Boolean vector indicating imputed values.
missing.data$x2 = imputed.x2 #Replacing the old vector.
mean(missing.data$x2) #Mean of x2 after imputation.
sd(missing.data$x2) #Standard deviation of x2 after imputation.
cor(missing.data, use = "complete.obs") #Correlation afterto imputation.
glimpse(missing.data)
plot(missing.data) #What are some potential problems with mean value imputation?
##################################
#####Simple Random Imputation#####
##################################
#Recreating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
mean(missing.data$x2, na.rm = TRUE) #Mean of x2 prior to imputation.
sd(missing.data$x2, na.rm = TRUE) #Standard deviation of x2 prior to imputation.
cor(missing.data, use = "complete.obs") #Correlation prior to imputation.
set.seed(0)
imputed.x2 = impute(missing.data$x2, "random") #Simple random imputation using the
#impute() function from the Hmisc package.
imputed.x2
summary(imputed.x2) #Summary information for the imputed variable.
is.imputed(imputed.x2) #Boolean vector indicating imputed values.
missing.data$x2 = imputed.x2 #Replacing the old vector.
mean(missing.data$x2) #Mean of x2 after imputation.
sd(missing.data$x2) #Standard deviation of x2 after imputation.
cor(missing.data, use = "complete.obs") #Correlation afterto imputation.
plot(missing.data) #What are some potential problems with mean value imputation?
#############################
#####K-Nearest Neighbors#####
#############################
#Recreating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
#Imputing using 1NN.
imputed.1nn = kNN(missing.data, k=1)
?kNN
library("VIM")
#Imputing using 1NN.
imputed.1nn = kNN(missing.data, k=1)
imputed.1nn
#############################
#####K-Nearest Neighbors#####
#############################
#Recreating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
#Imputing using 1NN.
imputed.1nn = kNN(missing.data, k=1)
imputed.1nn
#Imputing using 5NN.
imputed.5nn = kNN(missing.data, k=5)
imputed.5nn
#Imputing using 9NN.
imputed.9nn = kNN(missing.data, k=9)
imputed.9nn
### Imputing with caret
### Note: knnImpute with caret::preProcess force normalization
#Imputing using 1NN.
pre.1nn = preProcess(missing.data, method = 'knnImpute', k=1)
imputed.1nn = predict(pre.1nn, missing.data)
install.packages("RANN")
### Imputing with caret
### Note: knnImpute with caret::preProcess force normalization
#Imputing using 1NN.
pre.1nn = preProcess(missing.data, method = 'knnImpute', k=1)
imputed.1nn = predict(pre.1nn, missing.data)
#Imputing using 5NN.
pre.5nn = preProcess(missing.data, method = 'knnImpute', k=5)
imputed.5nn = predict(pre.5nn, missing.data)
#Imputing using 9NN.
pre.9nn = preProcess(missing.data, method = 'knnImpute', k=9)
imputed.9nn = predict(pre.9nn, missing.data)
imputed.1nn #Inspecting the imputed values of each of the methods;
imputed.5nn #what is going on here? Given our dataset, should we
imputed.9nn #expect these results?
#############################
#####K-Nearest Neighbors#####
#############################
#Recreating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
#Imputing using 1NN.
imputed.1nn = kNN(missing.data, k=1)
imputed.1nn
#Imputing using 5NN.
imputed.5nn = kNN(missing.data, k=5)
imputed.5nn
#Imputing using 9NN.
imputed.9nn = kNN(missing.data, k=9)
imputed.9nn
### Imputing with caret
### Note: knnImpute with caret::preProcess force normalization
#Imputing using 1NN.
pre.1nn = preProcess(missing.data, method = 'knnImpute', k=1)
imputed.1nn = predict(pre.1nn, missing.data)
imputed.1nn
### Imputing with caret
### Note: knnImpute with caret::preProcess force normalization
#Imputing using 1NN.
pre.1nn = preProcess(missing.data, method = 'knnImpute', k=1)
imputed.1nn = predict(pre.1nn, missing.data)
#Imputing using 5NN.
pre.5nn = preProcess(missing.data, method = 'knnImpute', k=5)
imputed.5nn = predict(pre.5nn, missing.data)
#Imputing using 9NN.
pre.9nn = preProcess(missing.data, method = 'knnImpute', k=9)
imputed.9nn = predict(pre.9nn, missing.data)
imputed.1nn #Inspecting the imputed values of each of the methods;
imputed.5nn #what is going on here? Given our dataset, should we
imputed.9nn #expect these results?
#K-Nearest Neighbors regression on the sleep dataset.
sqrt(nrow(sleep)) #Determining K for the sleep dataset.
#Using 8 nearest neighbors.
pre.8nn = preProcess(sleep, method = 'knnImpute', k=8)
sleep.imputed8NN = predict(pre.8nn, sleep)
summary(sleep) #Summary information for the original sleep dataset.
summary(sleep.imputed8NN[, 1:10]) #Summary information for the imputed sleep dataset.
#K-Nearest Neighbors classification on the iris dataset.
help(iris) #Inspecting the iris measurement dataset.
iris
iris.example = iris[, c(1, 2, 5)] #For illustration purposes, pulling only the
#Throwing some small amount of noise on top of the data for illustration
#purposes; some observations are on top of each other.
set.seed(0)
iris.example$Sepal.Length = jitter(iris.example$Sepal.Length, factor = .5)
iris.example$Sepal.Width = jitter(iris.example$Sepal.Width, factor= .5)
col.vec = c(rep("red", 50), #Creating a color vector for plotting purposes.
rep("green", 50),
rep("blue", 50))
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica"),
pch = 16, col = c("red", "green", "blue"), cex = .75)
missing.vector = c(41:50, 91:100, 141:150) #Inducing missing values on the Species
iris.example$Species[missing.vector] = NA  #vector for each category.
iris.example
col.vec[missing.vector] = "purple" #Creating a new color vector to
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica", "NA"),
pch = 16, col = c("red", "green", "blue", "purple"), cex = .75)
#Inspecting the Voronoi tesselation for the complete observations in the iris
#dataset.
library(deldir) #Load the Delaunay triangulation and Dirichelet tesselation library.
install.packages("deldir")
info = deldir(iris.example$Sepal.Length[-missing.vector],
iris.example$Sepal.Width[-missing.vector])
#Inspecting the Voronoi tesselation for the complete observations in the iris
#dataset.
library(deldir) #Load the Delaunay triangulation and Dirichelet tesselation library.
info = deldir(iris.example$Sepal.Length[-missing.vector],
iris.example$Sepal.Width[-missing.vector])
plot.tile.list(tile.list(info),
fillcol = col.vec[-missing.vector],
main = "Iris Voronoi Tessellation\nDecision Boundaries")
#Adding the observations that are missing species information.
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = 16, col = "white")
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = "?", cex = .66)
#Conducting a 1NN classification imputation.
iris.imputed1NN = kNN(iris.example, k = 1)
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed1NN$Species)
#Conducting a 12NN classification imputation based on the square root of n.
sqrt(nrow(iris.example))
iris.imputed12NN = kNN(iris.example, k = 12)
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed12NN$Species)
##################################################
#####Using Minkowski Distance Measures in KNN#####
##################################################
library(kknn) #Load the weighted knn library.
#Separating the complete and missing observations for use in the kknn() function.
complete = iris.example[-missing.vector, ]
missing = iris.example[missing.vector, -3]
#Distance corresponds to the Minkowski power.
iris.euclidean = kknn(Species ~ ., complete, missing, k = 12, distance = 2)
install.packages("kknn")
##################################################
#####Using Minkowski Distance Measures in KNN#####
##################################################
library(kknn) #Load the weighted knn library.
#Separating the complete and missing observations for use in the kknn() function.
complete = iris.example[-missing.vector, ]
missing = iris.example[missing.vector, -3]
#Distance corresponds to the Minkowski power.
iris.euclidean = kknn(Species ~ ., complete, missing, k = 12, distance = 2)
install.packages("kknn")
#Distance corresponds to the Minkowski power.
iris.euclidean = kknn(Species ~ ., complete, missing, k = 12, distance = 2)
summary(iris.euclidean)
iris.manhattan = kknn(Species ~ ., complete, missing, k = 12, distance = 1)
summary(iris.manhattan)
## Libraries used ####
library(tidyverse)
library(lubridate)
library(stringr)
library(purrr)
library(dplyr)
### Function to load a file and append to a dataframe ####
loadAndMerge = function(file_path,file2, temp_df1 = NULL){
print(paste0("Processing file Name : ",file2 ))
temp_df2 = read.csv(paste0(file_path,file2), stringsAsFactors = FALSE)
if(is.null(temp_df1)) {
return(temp_df2)
}
col_str1 = paste(colnames(temp_df1), collapse = "")
col_str2 = paste(colnames(temp_df2), collapse = "")
if(col_str1 == col_str2) {
return (rbind(temp_df1,temp_df2))
}
for (col_name in colnames(temp_df1)){
if (!col_name %in% colnames(temp_df2)) {
temp_df2[[col_name]] = NA
print(paste0("col added to temp_df2 : ", col_name))
}
}
for (col_name in colnames(temp_df2)){
if (!col_name %in% colnames(temp_df1)) {
temp_df1[[col_name]] = NA
print(paste0("col added to temp_df1 : ", col_name))
}
}
return (rbind(temp_df1,temp_df2))
}
### Functin to look for .csv files in folder and load them iteratively ####
multiFileMerge = function(my_path, file_pattern){
file_names <- list.files(path = my_path  ,pattern = file_pattern)
df = NULL
for (file_name in file_names)
df <- loadAndMerge(my_path, file_name, df)
return(df)
}
### Call the functin and load the filed in folder
combined_df = multiFileMerge("./data/raw/", ".csv")
combined_df
### Functin to look for .csv files in folder and load them iteratively ####
multiFileMerge = function(my_path, file_pattern){
file_names <- list.files(path = my_path  ,pattern = file_pattern)
print(file_names)
df = NULL
for (file_name in file_names)
df <- loadAndMerge(my_path, file_name, df)
return(df)
}
### Call the functin and load the filed in folder
combined_df = multiFileMerge("./data/raw/", ".csv")
### Call the functin and load the filed in folder
setwd("/Users/RV/Documents/Radha/OneDrive\ -\ Cognizant/Personal/Learn/DataScience/NYC/classes/handson/Projects/WebScraping/f500/")
combined_df = multiFileMerge("./data/raw/", ".csv")
write.csv(combined_df,"./data/processed/merged_data2.csv", row.names = FALSE)
